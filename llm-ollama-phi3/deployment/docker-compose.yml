version: "3.9"
services:
  llm_phi3_inference_util:
    container_name: llm_phi3_inference_util
    image: local/highwind-examples/llm-ollama-phi3:latest
    platform: linux/amd64
    command: --model_name=model
    working_dir: /app
    ports:
      - "8080:8080"