# Template details

## Overview
- This folder contains scripts encompassing the standard ML workflow set of steps essential for a ML project, including:
    - Loading and reading data
    - Data pre-processing
    - Feature engineering
    - Model training
    - Model evaluation
- Additionally, it includes a utilities script intended for methods that don't necessarily fit into the aforementioned script categories.

## Getting Started
1. Start by creating a folder structure for your project or copy the following folder `containerized_python_components`, which should include the following folders (see existing list of examples in the example section below as a reference):
    - `notebooks`: This contains the exploratory  notebooks
    - `deployment`: This will house all files, folders, scripts..etc for deployment
    - `saved_model` This will contain the saved model which can be created from your Notebook as part of saving the model
    - `training`: This will contain all files, folders, scripts..etc relating to the ML workflow process
2. After creating/copying the folder, you can proceed to create the notebooks for exploratory analysis and testing purposes within the `notebooks` folder.

3. Once you are satisfied with your project in the the notebook, proceed to the steps below.
    - Within each script, you will find boiler plate code and example methods.
    - Customize each script based on the methods and specifics of your project.
    - `NB`: Selectively copy and modify the scripts according to your project's requirements.


4. Proceed to populate the `pipeline.py` within the `training` folder file as follows:
    - `pipeline.py`: Import and utilize the functions defined in the development scripts (eg: `data_prep.py`, `model_evaluation`..etc.). See existing list of examples in the example section below as a reference.
    - Within the `pipeline.py` script, you can add `memory limits`. The max limit on Highwind is `6G`
    - For further details please refer to [Kubeflow Containerized Python Components](https://www.kubeflow.org/docs/components/pipelines/v2/components/containerized-python-components/)
    - A target image is required in order to add and use the additional ML scripts
    - Within the `pipeline.py script`, look for `@dsl.component`, below `base_image`, proceed to add a `target_image`
        - The target image will be the `Image URI` generated on Highwind when you create an Asset (See section below for steps on how to get the image URI). 
        - Example: `target_image="example_image:v1"`
        - **NB:** You might be prompted to install additional packages, please install what is required.
        - Next run the following command, to build the component locally: `kfp component build ./ --component-filepattern pipeline.py --no-push-image`
            -  **NB:** Everytime you want to run the `kfp component build` command, you are required to delete the files generated by KFP. Unfortunately, KFP does not overwrite these files when executing the mentioned command, hence the file deletion is required.
    - Once done, you should now see additional files and folders get generated by KFP:
        - `component_metadata` folder with a yaml file 
        - `kfp_config.ini` file
        - `.dockerignore` file
        - `Dockerfile`
        - `runtime-requirements` text file
    - Update the rest of the pipeline script according to your project requirements (use the existing examples as a reference)
    - You can then execute the script to compile the IR YAML file using (Please see `Kubeflow Caveats`):
    ```python
            python -m pipeline
           # A new yaml file is created.
    ```
5.  This file is the IR YAML file that can be uploaded to Highwind.

6. `Deployment folder`: Please see the `deployment` folder set-up for the examples listed below when making updates for deployment
    - For the `main.py` script, you are required to inlcude `predict` and `model load` functions
    - The rest of the files in the folder can be copied and adjusted to your project requirements

7. **OPTIONAL**: Local testing of the KFP pipeline
   - **NB:** Please see Highwind user manual for local testing steps
   - This step is optional, but if your pipeline runs successfully locally, you can then proceed to upload the generated IR YAML file to Highwind
   - Ensure you switch your kfp version from `2.7.0` back to `2.0.0b15` prior to generating the IR YAML file as local testing requires kfp version `2.7.0`.


## Kubeflow Caveats
- **Caveat 1 - Optional Args:** After compiling the IR YAML file, please ensure it does not contain `isOptional: true` as this will result in an invalid IR YAML.

- **Caveat 2 - Pipeline Description:** Before compiling the IR YAML file, please ensure within the `@dsl.pipeline` section, there is no description included as this will result in a failure to submit the IR YAML onto Highwind due to formatting issues.
    - This can also be resolved after compiling the IR YAML by searching for `description` which should appear within the section called `pipelineInfo` if it is present, then remove description and save the IR YAML.


## Python Containerised Examples
1. Iris Classification

## Getting the target image uri from Highwind
1. **Create an Asset - `Docker repo`**
    - Log onto `Highwind`
    - Navigate to `My Assets` on the left of the screen
    - Click on `+ Create Asset`
    - Select `Docker repo` for `Asset Type`
    - Add a `Cover image`
    - Provide a `Name` and `Description`
    - Finally, select `Create Asset` button

2. **Image push commands and Image URI**
 - Please refer to the [Highwind user manual](https://docs.highwind.ai/) within the `Tutorials` tab for essential steps on retrieving and executing the Docker push commands and obtaining the image URI.

 - After following the instructions from the user manual, proceed to copy the `Image URI` and add it to the target_image in your pipeline.py or if you're doing local testing then in your pipeline-local.py script

**NB**: You need two target images when testing locally and when generating your IR YAML file
 - Loacal testing image: The target image for the local `pipeline-local.py`script requires the KFP version to be `2.7.0`
 - IR YAML file: The target image within the `pipeline.py` script requires kfp version `2.0.0b15`


## Dependency Management
- The various examples makes use of poetry for dependency management
- For further details on poetry, please refer to [python poetry website](https://python-poetry.org/docs/)
- `NB:` You are free to use any dependency management tools help manage project dependencies and package installations.