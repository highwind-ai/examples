{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Translation Example\n",
        "\n",
        "This notebook provides the code to execute preprocessing, training, and inference for a translation model (from English to French) which is based on the following tutorial: https://huggingface.co/docs/transformers/tasks/translation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.40.2)\n",
            "Requirement already satisfied: datasets in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.19.1)\n",
            "Requirement already satisfied: evaluate in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (0.4.2)\n",
            "Requirement already satisfied: sacrebleu in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.4.2)\n",
            "Requirement already satisfied: tensorflow in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.16.1)\n",
            "Requirement already satisfied: tf-keras in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.16.0)\n",
            "Requirement already satisfied: filelock in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2024.4.28)\n",
            "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (16.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: portalocker in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sacrebleu) (5.2.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: setuptools in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorflow) (3.3.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: rich in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the line below to install the required libraries\n",
        "!pip install transformers datasets evaluate sacrebleu tensorflow tf-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sign in to your Hugging Face account\n",
        "\n",
        "This will enable you to upload and share the model.\n",
        "\n",
        "### Steps to get the `Access Token` from Hugging Face:\n",
        "\n",
        " - **Sign In or Sign Up:** If you don't have a Hugging Face account yet, you'll need to sign up. If you already have an account, sign in.\n",
        "\n",
        " - **Access Your Profile:** Once you're signed in, navigate to your profile settings. You can do this by clicking on your profile icon or username, usually located in the top-right corner of the Hugging Face website.\n",
        " \n",
        "- **Navigate to Access Token Settings:** Within your profile settings, look for an option related to Access tokens. This is where you can manage and generate tokens.\n",
        "\n",
        "- **Generate a New Token:** If you haven't generated a token before, you'll see a button (`New token`) to generate a new token. Click on this button.\n",
        "\n",
        "- **Name Your Token (Optional):** You may be prompted to give your token a name or description. This step is optional but can be helpful if you plan to generate multiple tokens for different purposes.\n",
        "\n",
        "- **Copy Your Token:** Once your token is generated, you'll typically see it displayed on the screen. Make sure to copy the token and replace it in the `login` code below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEujDoOqjSP1"
      },
      "source": [
        "# Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OSc96Qe7jSP3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /home/gitpod/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z207J-_yjSP3"
      },
      "source": [
        "## Load OPUS Books dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0CJRtJljSP3"
      },
      "source": [
        "Start by loading the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset from the 🤗 Datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zHTi_ef6jSP4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading readme: 100%|██████████| 28.1k/28.1k [00:00<00:00, 12.6MB/s]\n",
            "Downloading data: 100%|██████████| 21.0M/21.0M [00:01<00:00, 11.2MB/s]\n",
            "Generating train split: 100%|██████████| 127085/127085 [00:00<00:00, 687945.34 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': (127085, 2)}\n",
            "{'train': (500, 2), 'test': (50, 2)}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': '0', 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
        "\n",
        "books = load_dataset(\"opus_books\", \"en-fr\")\n",
        "print(books.shape)\n",
        "\n",
        "subset_train = books[\"train\"].select(range(500))\n",
        "subset_test = books[\"train\"].select(range(50))\n",
        "books = DatasetDict({\n",
        "    \"train\": subset_train,\n",
        "    \"test\": subset_test\n",
        "})\n",
        "\n",
        "print(books.shape)\n",
        "books[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwJyR3f3jSP4"
      },
      "source": [
        "Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QBclzyNjSP4"
      },
      "source": [
        "`translation`: an English and French translation of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlitwZ0njSP4"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMJTyBbQjSP4"
      },
      "source": [
        "The next step is to load a T5 tokenizer to process the English-French language pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WhEyFPjxjSP5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "/home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65OaaSOjSP5"
      },
      "source": [
        "The preprocessing function you want to create needs to:\n",
        "\n",
        "1. Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
        "2. Tokenize the input (English) and target (French) separately because you can't tokenize French text with a tokenizer pretrained on an English vocabulary.\n",
        "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FktrbP9KjSP5"
      },
      "outputs": [],
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"fr\"\n",
        "prefix = \"translate English to French: \"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
        "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nbYBYT9jSP5"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use 🤗 Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XqVH88sxjSP5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 500/500 [00:00<00:00, 7701.48 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 4147.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_books = books.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTIhvn4EjSP5"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BoEe7qg9jSP5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-09 11:25:32.356249: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-09 11:25:32.808183: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-05-09 11:25:33.316979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-09 11:25:35.063936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSnR7zQpjSP5"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ9MLFvLjSP5"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EOKdvqeIjSP5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 8.15k/8.15k [00:00<00:00, 11.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLKGkfkljSP5"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the SacreBLEU score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8cG43rHXjSP5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ7-XCEljSP5"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3BNouEGjSP5"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdUDEr9BjSP5"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with Keras, take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-a-tensorflow-model-with-keras)!\n",
        "\n",
        "</Tip>\n",
        "To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zy8rKsKyjSP5"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamWeightDecay\n",
        "\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIm8o3xAjSP5"
      },
      "source": [
        "Then you can load T5 with [TFAutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g9_-ukp7jSP5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEfhUV9njSP5"
      },
      "source": [
        "Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IEGLCAs6jSP5"
      },
      "outputs": [],
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_books[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_books[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foQMjU40jSP6"
      },
      "source": [
        "Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ig77Vod_jSP6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=optimizer)  # No loss argument!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81k4U-GvjSP9"
      },
      "source": [
        "Specify where to push your model and tokenizer in the [PushToHubCallback](https://huggingface.co/docs/transformers/main/en/main_classes/keras_callbacks#transformers.PushToHubCallback):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bxnwy4zkjSP9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gitpod/.pyenv/versions/3.12.3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/MelioAI/machine-translation into local empty directory.\n",
            "Download file tf_model.h5: 100%|██████████| 357M/357M [00:04<00:00, 91.6MB/s]\n",
            "Clean file tf_model.h5: 100%|██████████| 357M/357M [00:01<00:00, 321MB/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "# Callback that will save and push the model to the Hub \n",
        "push_to_hub_callback = PushToHubCallback(\n",
        "    output_dir=\"../tf_saved_model\",\n",
        "    tokenizer=tokenizer\n",
        "    #hub_model_id=\"MelioAI/machine-translation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc82-o-EjSP9"
      },
      "source": [
        "Then bundle your callbacks together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "x8pZzv8ejSP-"
      },
      "outputs": [],
      "source": [
        "\n",
        "callbacks = [push_to_hub_callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_model_path=\"../tf_saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make temporary local repo dir\n",
        "local_repo = Path(\"../hf-repo\") # mkdtemp(prefix=\"skops-\")\n",
        "\n",
        "if PUSH_TO_HF:\n",
        "\n",
        "    # Initialise HF repo\n",
        "    hub_utils.init(\n",
        "        model=Path(save_model_path),\n",
        "        requirements=[\n",
        "            f\"scikit-learn=={sklearn.__version__}\",\n",
        "            f\"joblib=={joblib.__version__}\"\n",
        "        ],\n",
        "        dst=local_repo,\n",
        "        task=\"tabular-classification\",\n",
        "        data=X_train.head(),\n",
        "        model_format=\"pickle\"\n",
        "    )\n",
        "\n",
        "    # Add feature scaler to repo\n",
        "    hub_utils.add_files(save_scaler_path, dst=local_repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "PUSH_TO_HF = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'card' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m PUSH_TO_HF:\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[39m# Create and populate basic model card\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model_card \u001b[39m=\u001b[39m card\u001b[39m.\u001b[39mCard(model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m      5\u001b[0m     metadata \u001b[39m=\u001b[39m card\u001b[39m.\u001b[39mmetadata_from_config(local_repo \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconfig.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Add model card detail\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'card' is not defined"
          ]
        }
      ],
      "source": [
        "if PUSH_TO_HF:\n",
        "\n",
        "    # Create and populate basic model card\n",
        "    model_card = card.Card(model=model)\n",
        "    metadata = card.metadata_from_config(local_repo / \"config.json\")\n",
        "    \n",
        "    # Add model card detail\n",
        "    limitations = (\n",
        "        \"This model is made for the purposes of showing how to use Highwind only.\"\n",
        "    )\n",
        "    model_description = (\n",
        "        \"This is a machine translation model\"\n",
        "        \"used to translate from English to French. This model\"\n",
        "        \" is to only be used as an example of how to use Highwind.\"\n",
        "\n",
        "        \n",
        "    )\n",
        "    model_card_authors = \"MelioAI, Verosha08\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1eeVrCyjSP-"
      },
      "source": [
        "Finally, you're ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlm5jg4EjSP-"
      },
      "outputs": [],
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"my_awesome_opus_books_model_tf\")\n",
        "model.push_to_hub(\"my_awesome_opus_books_model_tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zitFhbRvjSP-"
      },
      "source": [
        "Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n",
        "\n",
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrUIxcpmjSP-"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9j3vbadjSP-"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Come up with some text you'd like to translate to another language. For T5, you need to prefix your input depending on the task you're working on. For translation from English to French, you should prefix your input as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcwcDplqjSP-"
      },
      "outputs": [],
      "source": [
        "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqFIpotjSP-"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for translation with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K6B-JH8jSP-",
        "outputId": "7e0250f2-a711-482f-c7a2-5c4ba44fa667"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Merelda/my_awesome_opus_books_model_tf\")\n",
        "translator(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8surtifjSP-"
      },
      "source": [
        "# Manual Inference\n",
        "\n",
        "Once you train the model, this is how you can test it without using Huggingface Hub.\n",
        "\n",
        "1. Save the tokenizer and model manually\n",
        "2. You can either use the `pipeline` function, *OR*\n",
        "3. Construct the `pipeline` manually with \n",
        "  - Tokenize the text and return the input_ids as tensors.\n",
        "  - Use the generate() method to create the translation. \n",
        "  - Decode the generated token ids back into text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"machine-translation/tf_saved_model\")\n",
        "model.save_pretrained(\"machine-translation/tf_saved_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "## Do everything in one step using pipeline\n",
        "translator = pipeline(\"translation\", model=\"machine-translation/tf_saved_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYyr9xe3jSP-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"machine-translation/tf_saved_model\")\n",
        "loaded_model = TFAutoModelForSeq2SeqLM.from_pretrained(\"machine-translation/tf_saved_model/\")\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n",
        "outputs = loaded_model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Save to Hugging Face Hub\n",
        "Save trained model files to the Hugging Face Hub so that they can be downloaded later. In this step, we use the useful helper functions provided by the [`skops`](https://github.com/skops-dev/skops/tree/main) package.\n",
        "\n",
        "If `PUSH_TO_HF` is enabled (see top of this notebook), this section will execute. Remember to log into Hugging Face with the CLI by running: `huggingface-cli login` otherwise this section won't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make temporary local repo dir\n",
        "local_repo = Path(\"../hf-repo\") # mkdtemp(prefix=\"skops-\")\n",
        "\n",
        "if PUSH_TO_HF:\n",
        "\n",
        "    # Initialise HF repo\n",
        "    hub_utils.init(\n",
        "        model=Path(save_model_path),\n",
        "        requirements=[\n",
        "            f\"scikit-learn=={sklearn.__version__}\",\n",
        "            f\"joblib=={joblib.__version__}\"\n",
        "        ],\n",
        "        dst=local_repo,\n",
        "        task=\"tabular-classification\",\n",
        "        data=X_train.head(),\n",
        "        model_format=\"pickle\"\n",
        "    )\n",
        "\n",
        "    # Add feature scaler to repo\n",
        "    hub_utils.add_files(save_scaler_path, dst=local_repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PUSH_TO_HF:\n",
        "\n",
        "    # Create and populate basic model card\n",
        "    model_card = card.Card(model=model)\n",
        "    metadata = card.metadata_from_config(local_repo / \"config.json\")\n",
        "    \n",
        "    # Add model card detail\n",
        "    limitations = (\n",
        "        \"This model is made for the purposes of showing how to use Highwind only.\"\n",
        "    )\n",
        "    model_description = (\n",
        "        \"This is a linear regression model trained on California housing dataset. This model could be\"\n",
        "        \" used to predict median price of a house in California, given certain features. This model is very basic and\"\n",
        "        \" should only be used as an example of how to use Highwind.\"\n",
        "    )\n",
        "    model_card_authors = \"MelioAI, ruanmelio\"\n",
        "    usage_code = \"\"\"\n",
        "```python\n",
        "import joblib\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Feature scaler\n",
        "hf_hub_download(\"MelioAI/california-housing\", \"scaler.joblib\")\n",
        "scaler = joblib.load(\"scaler.joblib\")\n",
        "\n",
        "# Classifier model\n",
        "hf_hub_download(\"MelioAI/california-housing\", \"model.joblib\")\n",
        "model = joblib.load(\"model.joblib\")\n",
        "```\n",
        "\"\"\"\n",
        "    model_card.add(\n",
        "        folded=False,\n",
        "        **{\n",
        "            \"Model Card Authors\": model_card_authors,\n",
        "            \"Intended uses & limitations\": limitations,\n",
        "            \"Model description\": model_description,\n",
        "            \"Model description/Intended uses & limitations\": limitations,\n",
        "            \"How to Get Started with the Model\": usage_code\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Add tags\n",
        "    model_card.metadata.library_name = \"sklearn\"\n",
        "    model_card.metadata.tags = [\"sklearn\", \"tabular-regression\"]\n",
        "\n",
        "    # Save model card\n",
        "    model_card.save(local_repo / \"README.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remember to log into HF with the CLI by running: huggingface-cli login\n",
        "PUSH_TO_HF = True # Whether to push to Hugging Face Hub or not\n",
        "HF_REPO_NAME = \"MelioAI/machine-translation\" # For pushing model to Hugging Face Hub \n",
        "\n",
        "if PUSH_TO_HF:\n",
        "\n",
        "    # Push to HF Hub\n",
        "    hub_utils.push(\n",
        "        repo_id=HF_REPO_NAME,\n",
        "        source=local_repo\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_REPO_NAME = \"MelioAI/machine-translation\"\n",
        "model_dir = \"../tf_saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 7 files: 100%|██████████| 7/7 [00:15<00:00,  2.25s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/workspace/examples/machine-translation/tf_saved_model'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download model\n",
        "snapshot_download(\n",
        "    repo_id=HF_REPO_NAME,\n",
        "    local_dir=model_dir\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
